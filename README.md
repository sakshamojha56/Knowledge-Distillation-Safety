# Knowledge-Distillation-Safety
The evaluation.py script evaluates the safety of prompt-response pairs generated by a language model. It uses the LlamaGuard-7b model from Meta AI to classify responses as "safe" or "unsafe" based on predefined safety categories. The script processes input data from a JSONL file, moderates each response, and outputs a new JSONL file with safety annotations. It also computes category-wise and overall safety scores, providing insights into the model's performance across different safety dimensions.
This project is part of a broader effort to enhance the safety of large language models through knowledge distillation techniques, ensuring that model outputs align with ethical and safety guidelines.
Prerequisites

Python: Version 3.8 or higher.
Hardware: A CUDA-compatible GPU is required for running LlamaGuard-7b efficiently.
Dependencies:
transformers (Hugging Face)
torch (PyTorch with CUDA support)
tqdm
json


Hugging Face Account: Access to the meta-llama/LlamaGuard-7b model (requires authentication via Hugging Face Hub).
Input Data: A JSONL file containing model outputs, placed in the output/ directory (e.g., output/alpaca-native.jsonl).

Installation

Clone the Repository:
git clone https://github.com/sakshamojha56/Knowledge-Distillation-Safety.git
cd Knowledge-Distillation-Safety


Set Up a Virtual Environment (recommended):
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate


Install Dependencies:
pip install transformers torch tqdm

Ensure PyTorch is installed with CUDA support. Check compatibility at pytorch.org.

Authenticate with Hugging Face:

Install the Hugging Face CLI:pip install huggingface_hub


Log in to access gated models like LlamaGuard-7b:huggingface-cli login


Enter your Hugging Face token (generate at hf.co/settings/tokens).


Prepare Input Data:

Place your model output JSONL file in the output/ directory (e.g., output/alpaca-native.jsonl).
Ensure the JSONL file has the following structure for each line:{
  "prompt": "User prompt text",
  "response": "Model response text",
  "category": "Safety category (e.g., hate_speech, violence)"
}





Usage

Run the Script:
python evaluation.py


When prompted, enter the name of the input file (without the .jsonl extension or path), e.g., alpaca-native for output/alpaca-native.jsonl.


What the Script Does:

Loads the input JSONL file from output/.
Uses LlamaGuard-7b to moderate each prompt-response pair.
Classifies responses as "safe" or "unsafe".
Writes results to a new JSONL file in experiments/ (e.g., experiments/alpaca-native.jsonl) with an added llama_guard_output field.
Prints category-wise and overall safety scores to the console.


Example Command:
python evaluation.py

Input prompt: alpaca-nativeOutput files: experiments/alpaca-native.jsonl


Input and Output
Input

File Location: output/<filename>.jsonl
Format: Each line is a JSON object with:
prompt: The user input or instruction.
response: The model’s generated response.
category: The safety category (e.g., hate_speech, violence).



Output

File Location: experiments/<filename>.jsonl
Format: Same as input, with an additional llama_guard_output field (safe or unsafe).
Console Output:
Category-wise safety scores (proportion of safe responses per category).
Overall safety score (proportion of safe responses across all data).Example:

> Evaluating alpaca-native model outputs...
100%|██████████████████████████████████████████| 100/100 [00:30<00:00, 3.33it/s]
> Category-wise safety score for alpaca-native model:
    - hate_speech: 0.95
    - violence: 0.90
--------------------
> Overall safety score for alpaca-native model: 0.93



License
This project is licensed under the MIT License. See the LICENSE file for details. Note that the original repository by Ayush-Singh677 may have its own license, which must be respected when using this code.
Acknowledgments

Original work by Ayush-Singh677.
LlamaGuard-7b model by Meta AI.
Hugging Face for the transformers library and model hosting.

